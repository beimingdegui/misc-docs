:stem: latexmath

== 领域专用体系结构

针对特定领域定制处理器，加速某些应用程序以实现更好的性能与性价比

登纳德缩放比例定律的终结远早于摩尔定律。因此，晶体管开关越多，意味着功耗越高。能量预算并没有增加，而且我们已经将单个低效处理器替换为多个高效核。因此，我们已经囊中乏计，无法再继续大幅提升通用体系结构的性价比和能效了。因为能量预算是有限的（原因在于芯片的电迁移、机械和热限制），所以要想提升性能（每秒内执行更多的操作），就需要减少单次操作所消耗的能量。

按照一个算术运算指令的开销来计算，对现有处理器核进行微小的调整，可能会得到10%的提升，但如果希望在保持可编程性的同时得到指数级的提升，就需要将每条指令的算术运算操作的数量由1增加到数百。为实现这一级别的效率，就需要对计算机体系结构进行巨大的改变，由通用核变为领域专用体系结构(domain-specific architecture,DSA)。

于是，就像在过去10年里，迫不得已由单处理器转向多处理器一样，架构师们现在研究DSA也是因为对过往技术的绝望。现在的新常态是，一台计算机将包含标准处理器和领域专用处理器，其中标准处理器用来运行诸如操作系统之类的传统大型程序，领域专用处理器则仅执行非常有限的一些任务，但其执行效果极好。因此，与过去的同构多核芯片相比，这些计算机的异构性会高得多。

过去几十年利用摩尔定律进行的体系结构方面的创新（缓存、乱序执行等）也许并不能很好地与某些领域相匹配（特别是就能量利用而言），因此，可以重新利用这部分资源使芯片更符合领域需求。例如，缓存对于通用体系结构来说是非常出色的，但对于DSA来说就不一定了；有些应用程序的访存模式很容易预测，有些大数据集（比如视频）几乎没有数据重用，对于这些情况，多级缓存技术就显得大材小用了。因此，DSA的前景既包括提升硅的利用率，也包括提升能效，而在今天，后者通常更重要。

架构师很可能不会为了一个大型的C++程序（比如SPEC2017基准测试中的某个编译器）开发一个DSA。领城专用算法大多是针对较大系统中的小型计算密集型内核设计的，比如目标识别或语音理解。DSA应当专注于某一个任务子集，而不是准备运行整个程序。此外，改变基准测试的代码不再是违规；对DSA来说，这是一种非常有效的加速方法。于是，对DSA感兴趣的架构师想有所建树的话，必须现在就开始学习相关应用领域及算法。

除了拓展专业知识，领域专用架构师面对的另一个挑战是找到一个合适的目标领域，其需求大到有必要在一个SOC上为其分配专门的资源，甚至是专门研发一种定制芯片。定制芯片及其配套软件的非重复性工程（nonrecuring engincering,NRE）成本要分摊到生产的所有芯片上，因此，如果你只需要1000个芯片，那么成本将非常高。

对于小体量的应用程序，一种应对方法是使用可重配置的芯片，比如FPGA，这是因为它们的NRE成本低于定制芯片，而且几种不同的应用可以重复利用同一个可重配置的硬件，从而分摊其成本。然而，由于这种硬件的效率低于定制芯片，所以由FPGA得到的收益有限。

DSA的另一个挑战是软件移植。人们熟悉的编程环境（比如C++编程语言和编译器）很少能在 DSA上直接使用。

=== DSA指导原则

下面介绍DSA设计的5条基本原则，后文所说的4种DSA设计就是以这5条原则为指导的。遵循这5条基本原则不仅可以提高面积效率和能效，还有两个好处。第一，它们可以简化设计，从而降低 DSA的NRE成本。第二，对于DSA中常见的面向用户的应用程序来说，相较于传统处理器采用的时变性能优化方法，遵循这些基本原则的加速器可以更好地满足第99百分位响应时间期限。

1. 使用专用存储器将数据移动距离缩至最短。通用微处理器中的多级缓存使用了大量的硅面积和能量，试图以最佳方式为程序移动数据。例如，一个两路组相联缓存使用的能量是与之等价的软件控制便笺式存储器的2.5倍。当然。DSA的编译器编写者和程序员对其领域有着深刻的理解，所以不需要硬件来为他们移动数据。而是利用专门为该领域内的特定功能所定制的软件控制存储器来减少数据移动。
	
2. 将通过减少微体系结构高级优化措施所节省的资源，投入到更多的算术运算单元或更大的存储器中。架构师们将摩尔定律带来的好处转化为针对CPU和GPU的资源密集型优化（乱序执行、多线程、多重处理、预取、地址接合，等等）。鉴于架构师对这些领域中的程序执行有深刻的理解，这些资源最好投入到更多的处理单元或者更大的片上存储上。

3. 使用与该领域相匹配的最简并行形式。DSA的目标领城几乎总是有内在的并行性。所以一个DSA的关键决策就是如何充分利用其并行性，以及如何向软件展现这一特性。要围绕该领域固有的并行粒度来设计DSA，并在编程模型中简单地展现这一并行性。例如，就数据级并行而言，如果SIMD在该领域就够用了，那么对程序员和编译器编写者来说，它当然要比MIMD更容易。同样，如果VLIW可以表达该领域的指令级并行，那么与乱序执行相比，其设计规模可以更小，能效可以更高。

4. 缩小数据规模，减少数据类型，使之能满足该领域最低需求即可。许多领域中的应用程序通常都是受存储器限制的，所以可以利用更小位宽的数据类型来提高有效存储带宽和片上存储利用率。更短小、更简单的数据还允许你在同样的芯片面积上放置更多的算术运算单元。

5. 使用一种领域专用编程语言将代码移植到 DSA。DSA的一个经典难题是让应用程序在你的新体系结构上运行。一个长期存在的谬误是，假定你的新计算机非常有吸引力，以至于程序员们为了你的硬件而重写自己的代码。所幸，在架构师被迫将注意力转移到DSA之前，领域专用编程语言就已经流行起来。用于视觉处理的Halide和用于DNN的TensorFlow[Ragan-Kelley等，2013；Abadi等，2016]都是这方面的例子。这些语言大幅提高了向DSA移植应用程序的可行性。如前所述，在某些领域，应用程序中只有一些涉及大量计算的部分需要在DSA上运行，这也简化了代码移植。

下表说明了这四种DSA是如何遵守这些指导原则的。

[cols="1,1,1,1,1",options="header"]
|===
| 指导原则 | TPU | Catapult | Crest | Pixel Visual Core

| 设计目标
| 数据中心 ASIC
| 数据中心 FPGA
| 数据中心 ASIC
| PMD ASIC/SOC IP

| 1. 专用存储器
| 24 MiB 统一缓冲区，4 MiB 累加器
| 可变
| 无
| 每个核：128 KiB 行缓冲区，64 KiB P.E. 存储器

| 2. 更大的算术运算单元
| 65,536 个乘法累加器
| 可变
| 无
| 每个核：256 个乘法累加器（512 个 ALU）

| 3. 简单的并行机制
| 单线程，SIMD，顺序
| SIMD, MISD
| 无
| MPMD, SIMD, VLIW

| 4. 更大的数据规模
| 8 位、16 位整数
| 8 位、16 位整数，32 位浮点数
| 21 位浮点数
| 8 位、16 位、32 位整数

| 5. 领域专用语言
| TensorFlow
| Verilog
| TensorFlow
| Halide/TensorFlow
|===

=== 示例领域：深度神经网络

深度神经网络（Deep Neural Network, DNN）是人工神经网络的一种扩展形式，通过增加网络的层数和复杂度以实现对复杂数据特征的自动提取和学习。它是深度学习的核心技术，广泛应用于语音识别、图像处理、自然语言处理和推荐系统等领域。

深度神经网络由多层神经元组成，包括输入层、隐藏层和输出层。输入层负责接收原始数据，隐藏层通过逐层抽象和计算提取数据的高阶特征，输出层则生成预测结果或分类标签。隐藏层是DNN的核心，它由多个非线性激活函数、权重矩阵和偏置组成，通过优化训练获得表示能力。

DNN的特点是其深度结构和强大的特征学习能力。相比于传统的浅层网络，深度神经网络能够在隐藏层中捕获数据的复杂模式和高维特征，减少对手工设计特征的依赖。同时，通过使用反向传播算法，DNN能够有效地优化权重和偏置参数，从而提升模型性能。

DNN的训练过程通常需要大量标注数据和计算资源。通过前向传播计算网络输出，再通过反向传播计算误差和梯度，更新权重。优化器（如SGD或Adam）用于加速训练过程。为了解决深层网络容易出现的梯度消失或梯度爆炸问题，通常采用规范化技术（如批量归一化）和特殊激活函数（如ReLU）。

深度神经网络的成功得益于三大关键因素：大规模数据、强大的计算能力（如GPU、TPU）以及高效的训练算法。虽然DNN具有很高的表达能力，但其也面临着一些挑战，如模型过于复杂可能导致过拟合、训练时间较长、对高质量数据的依赖等。

==== DNN的神经元

深度神经网络（DNN）中的神经元是其基本构建单元，模拟生物神经元的行为，用于接收输入信号、执行计算并产生输出。每个神经元通过简单的数学运算将输入映射为输出，进而通过多层的连接构建起整个网络的计算能力。

一个神经元的主要组成部分包括输入、权重、偏置、激活函数和输出：

1. 输入：神经元从前一层的神经元接收输入数据，这些输入可能是原始数据特征或前一层的计算结果。每个输入与相应的权重配对。

2. 权重：权重是神经元学习的核心参数，用于衡量输入的相对重要性。在训练过程中，通过优化算法（如梯度下降）调整权重以最小化预测误差。

3. 偏置：偏置是一个额外的参数，与输入无关，能够调整激活函数的输出范围，确保网络对不同输入数据具有更好的拟合能力。

4. 加权和计算：神经元对所有输入进行加权求和，并加入偏置，计算公式为：
+

[stem]
++++
z = \sum_{i=1}^n w_i x_i + b

，其中x_i是第i个输入，w_i是对应的权重，b是偏置。
++++

5. 激活函数：激活函数对加权和结果 \(z\) 进行非线性变换，产生神经元的输出。常见的激活函数包括：

   - ReLU（修正线性单元）：stem:[\text{ReLU}(z) = \max(0, z)]
   - Sigmoid：stem:[\sigma(z) = \frac{1}{1 + e^{-z}}]
   - Tanh：stem:[\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}]

   激活函数的非线性特性使得神经网络能够表示复杂的非线性关系，提高网络的表达能力。

6. 输出：激活函数的结果作为神经元的输出，传递到下一层神经元。

在DNN中，大量神经元通过层与层之间的连接形成复杂的网络结构。每一层的神经元共同处理上一层的输出，并将计算结果传递给下一层，从而逐步提取数据的特征。神经元的数量和激活函数的选择对网络性能有显著影响。

通过反向传播算法，神经元的权重和偏置会根据损失函数的梯度逐步调整，从而使网络能够学习到输入数据的特征模式并完成预测或分类任务。神经元的设计与优化是DNN训练和推理性能的关键。

==== 训练与推理

深度神经网络（DNN）的训练与推理是其实现功能的两个核心过程，分别对应于模型的学习阶段和实际应用阶段。以下是对这两个过程的详细介绍：

DNN的训练
训练是DNN学习数据特征并优化模型参数的过程，主要包括以下几个关键步骤：

- 前向传播：在训练过程中，输入数据通过神经网络逐层传递，依次经过每一层的加权求和、偏置加和、激活函数处理，最后生成输出（预测值）。这一过程称为前向传播，用于计算模型的输出和与目标值之间的误差。

- 计算损失函数：损失函数用于衡量模型预测值与真实值之间的差距。例如，分类任务中常用交叉熵损失函数，回归任务中常用均方误差（MSE）。损失函数的值越小，说明模型的预测性能越好。

- 反向传播：反向传播通过链式法则计算损失函数对模型中所有可训练参数（权重和偏置）的梯度。从输出层开始，逐层向输入层回溯，通过求导计算每一层参数对损失的影响。反向传播是训练阶段的重要步骤，它将误差信号传播到整个网络以指导参数的调整。

- 参数更新：在获得梯度后，利用优化算法（如随机梯度下降SGD、Adam、RMSProp等）更新权重和偏置。优化算法根据学习率调整参数，使得模型的损失逐渐减小，网络逐渐逼近全局或局部最优解。

- 迭代训练：上述过程会在多个训练样本上反复进行，通常分为多个epoch（一个epoch表示整个训练集被网络完整训练一次）。通过多次迭代，模型逐步学习数据特征并提高性能。

训练过程的目标是通过反复优化使网络的权重和偏置达到能够有效捕捉数据分布和特征的状态，从而使网络能够泛化到未见过的测试数据。

DNN的推理
推理是DNN在训练完成后应用于实际任务的过程，主要用于对新数据进行预测。推理过程相对简单，与训练阶段的前向传播类似，但不涉及梯度计算和参数更新。其主要步骤包括：

- 输入数据预处理：在推理阶段，将实际输入数据按照训练时相同的方式进行预处理，例如归一化、标准化或图像缩放，以确保数据分布一致。

- 前向传播计算：输入数据经过神经网络逐层传递，网络的权重和偏置保持不变，最终输出预测结果。

- 输出结果：根据网络输出的值，得到分类标签、回归值或其他形式的预测结果。例如，在分类任务中，输出层可能提供每个类别的概率值，然后选取概率最高的类别作为预测结果。

推理过程的效率取决于网络的复杂性（如层数、参数量）和硬件性能。在实际应用中，推理阶段通常运行在高效的硬件（如GPU、TPU或专用加速器）上，以满足实时性需求。

==== 多层感知机

多层感知机（MLP，Multilayer Perceptron）是深度神经网络（DNN）的基础结构之一，也是最早发展起来的一种前馈神经网络。它由输入层、一个或多个隐藏层以及输出层组成，各层神经元之间全连接，但层内没有连接。MLP主要用于解决非线性问题，是深度学习中广泛使用的模型。以下是多层感知机的详细介绍：

1. 基本结构
+

多层感知机由多个相互堆叠的全连接层构成，每一层的输出作为下一层的输入。它包含以下几部分：

* 输入层：输入层的神经元个数由输入数据的特征维度决定，例如输入是一个向量，则输入层神经元个数等于向量的维度。

* 隐藏层：隐藏层是多层感知机的核心部分，可以有一个或多个隐藏层。每一层神经元通过加权求和（权重和偏置）处理输入数据，并通过激活函数引入非线性。隐藏层的数量和每层的神经元个数是模型设计的超参数，需要根据任务复杂度和数据特性进行调整。

* 输出层：输出层的神经元个数取决于具体的任务。例如，在分类问题中，输出层神经元的数量等于类别数；在回归问题中，输出层通常只有一个神经元。输出层的激活函数通常与任务类型相关，例如分类任务常用softmax或sigmoid函数，而回归任务常用线性激活函数。

2. 工作原理
+

多层感知机通过前向传播和反向传播实现输入到输出的映射，并通过训练调整参数以最小化预测误差。

- 前向传播：输入数据从输入层开始，逐层通过隐藏层计算，直到输出层生成结果。每个神经元的输出是输入的加权和加上偏置，再通过激活函数引入非线性。

- 反向传播：通过损失函数计算输出结果与真实值之间的误差，然后利用梯度下降算法（如SGD、Adam）通过链式法则计算误差对每一层参数的梯度，逐层更新权重和偏置以减少误差。

3. 激活函数的作用
+

隐藏层中的激活函数是多层感知机引入非线性的关键，它使得模型能够拟合复杂的非线性关系。如果没有激活函数，MLP只能表示线性映射，无法解决复杂的实际问题。常用的激活函数包括： 
+

* Sigmoid：将输出压缩到 (0, 1) 范围内，适用于概率预测，但在深层网络中可能导致梯度消失问题。

* ReLU（Rectified Linear Unit）：将负值输出为0，正值保持不变，解决了梯度消失问题，是现代深度学习中最常用的激活函数之一。

* Tanh：将输出压缩到 (-1, 1) 范围内，收敛速度比Sigmoid更快，但仍可能存在梯度消失问题。

4. MLP的特点

* 全连接结构：多层感知机中的每一层神经元与下一层神经元全连接，使其具有较强的特征表达能力。

* 非线性映射：通过激活函数，MLP能够学习输入和输出之间的复杂非线性关系。

* 参数数量多：由于每层神经元全连接，MLP的参数量较多，容易导致计算开销大和过拟合问题。

5. 应用场景
+

多层感知机是DNN最基本的形式，适用于多种任务，包括：

* 分类：用于图像、文本或其他形式数据的分类任务，例如手写数字识别。

* 回归：在预测连续值的任务中（如房价预测），MLP也能表现出色。

* 特征学习：可以用作其他复杂网络的基础层，通过学习数据的高维特征表示。

6. 局限性
+

尽管多层感知机是DNN的重要组成部分，但它也存在一些局限性：

* 不适合处理高维数据：例如图像数据，直接使用MLP会导致参数量过大，因此更适合卷积神经网络（CNN）。

* 全连接导致计算冗余：所有神经元之间的连接可能会造成资源浪费和冗余计算。

* 对时序数据支持不足：在处理时间序列数据时，MLP无法捕获时间上的依赖关系，因此通常使用循环神经网络（RNN）或Transformer模型。

==== 批数据

批数据是指在深度神经网络训练和推理过程中，将一组输入数据同时送入网络进行计算的方法。它通过批量处理多个样本来有效利用硬件的并行计算能力，提升训练效率，稳定梯度更新，并改善模型性能。在训练过程中，批数据的核心思想是将训练数据划分为多个小批次，每次使用一个批次的数据进行前向传播和反向传播。批数据方法是小批量梯度下降的重要实现方式，每个批次的数据量称为批大小，通常为2的幂，如32、64、128等，便于硬件高效计算。通过批量梯度下降，每个批次的数据用于计算损失函数和梯度，再用于更新模型参数。

批数据在深度神经网络中的作用主要体现在几个方面。它能够充分利用硬件（如GPU或TPU）的并行计算能力，加速矩阵运算（例如前向传播和反向传播中的加权求和和激活函数计算），显著提高计算效率。同时，相较于单样本更新的随机梯度下降，使用批数据能够减小梯度更新的随机性，使梯度估计更加平稳，有助于更快收敛。批数据还能减少内存开销，与全量梯度下降需要一次加载所有样本不同，批处理方法通过小批量迭代计算降低了对内存的需求。此外，批数据也能一定程度上避免过拟合，因为每次训练只用部分数据，模型不会完全依赖整个训练数据集。

批大小是批数据的关键参数，直接影响训练过程的效率和效果。小批大小（如16或32）更新频繁，计算更加精细，但可能会引入更多噪声，导致收敛速度变慢甚至不稳定，适合小型数据集或硬件资源有限的情况。大批大小（如256或更大）能提高计算效率，梯度估计更加准确，但可能导致模型陷入局部最优，适合大型数据集和资源充足的硬件场景。极端情况下，当批大小为1时，相当于随机梯度下降；当批大小等于数据集大小时，则为全量梯度下降。

在训练过程中，批数据的处理主要包括数据集划分、前向传播、损失计算、反向传播和参数更新。整个数据集被划分为若干批次，每个批次依次输入模型进行计算，得到预测结果并计算损失值，再通过反向传播计算梯度，用优化算法更新模型参数。批数据不仅在训练阶段具有重要作用，在推理阶段也能显著提升计算效率。在推理过程中，批处理的样本数根据硬件能力和任务需求选择合适的大小，例如在GPU上利用批数据可以最大化硬件利用率，减少推理时间；而对于实时性要求较高的任务，可能需要逐个样本推理。

总体而言，批数据是深度神经网络训练和推理中的重要概念，通过划分小批量样本实现了计算效率和训练效果的平衡。适当选择批大小能够最大化硬件资源利用率，同时优化模型的性能和稳定性，在深度学习的实际应用中发挥着重要作用。

=== Google的张量处理单元——一种数据中心推理加速器

==== TPU的概念

TPU（Tensor Processing Unit）是一种专为加速深度学习任务而设计的专用硬件加速器，由谷歌（Google）开发。它是一个面向人工智能和机器学习，特别是深度神经网络模型计算的专用集成电路（ASIC）。TPU的设计初衷是满足高性能计算需求，优化深度学习中大规模矩阵运算（如张量运算），以提高效率并降低能耗。相比于传统的CPU和GPU，TPU专注于为机器学习框架（如TensorFlow）提供高效支持。

TPU的特点主要体现在以下几个方面：首先，TPU以矩阵运算为核心，集成了大量专门用于矩阵乘法和累加的硬件单元，能够极大地提升张量运算的速度，是深度学习模型计算的核心。其次，TPU采用低精度计算单元（如8位整数或16位浮点数），从而在计算效率和内存带宽之间达到平衡，满足深度学习对大规模数据处理的需求，同时降低功耗。TPU的硬件架构简单，去除了许多通用处理器中的复杂控制逻辑和缓存系统，从而进一步提升了性能，并显著降低了硬件设计的成本和复杂度。此外，TPU还支持强大的并行计算能力，通过内置大量计算单元实现大规模并行运算，能够高效处理深度神经网络的训练和推理任务。

TPU的用途集中在深度学习任务中，广泛应用于训练和推理阶段。训练阶段中，TPU能够大幅度加速深度学习模型的优化过程，特别是在需要处理海量数据和复杂网络结构的场景下表现尤为突出，例如自然语言处理（NLP）、计算机视觉、语音识别和推荐系统等领域。推理阶段中，TPU可以在低延迟和高效率的前提下部署模型，用于实时任务或大规模数据处理，如搜索引擎优化、自动翻译、语音助手和视频推荐等应用。此外，TPU作为谷歌云服务的重要组成部分，还为开发者提供了广泛的云计算支持，使用户可以通过谷歌云平台高效运行机器学习任务，降低硬件门槛。

==== TPU体系结构

TPU（Tensor Processing Unit）的体系结构是一种专为深度学习设计的高效硬件架构，它通过简化的控制逻辑、高度并行化的计算单元和优化的数据流设计，为深度神经网络（DNN）计算提供了高性能支持。TPU的架构紧密围绕深度学习的核心需求，特别是矩阵运算（如张量计算），并在计算密集型任务中表现出色。

TPU的核心组件是其矩阵乘法单元（Matrix Multiply Unit，MXU），这是一种专门用于处理矩阵乘法的硬件单元。在第一代TPU中，MXU是一个支持 256×256 大小矩阵运算的单元，它能够高效执行大规模矩阵乘法和累加操作（如卷积运算），这是深度学习模型训练和推理的核心操作。通过高度优化的MXU，TPU实现了大规模并行计算，显著提升了深度学习的性能。

TPU架构中使用了低精度计算单元，例如8位整数（INT8）或16位浮点数（bfloat16），以减少计算复杂度和内存带宽需求，同时仍然满足神经网络模型的计算精度需求。这种低精度运算能够在不显著影响模型性能的情况下，大幅度提升计算速度和能效。

在数据存储方面，TPU引入了专用的高带宽存储器（如SRAM），称为“统一缓冲区”（Unified Buffer）。这个存储器用于缓存模型参数和中间数据，从而最大限度地减少与外部内存之间的数据传输开销。TPU还配备了片上内存（On-Chip Memory），用于支持高效的数据复用，并进一步优化数据流。通过这种设计，TPU能够以更高的效率处理深度学习的海量数据需求。

TPU的指令集相对简单，去除了传统处理器中的复杂控制逻辑。它采用了一种数据流驱动的计算模式，极大地减少了指令调度和控制的复杂性。这种架构设计能够更好地支持深度学习任务中规则化和高度并行的计算模式。

在输入输出（I/O）方面，TPU通过高速互连网络与其他硬件模块进行通信，以实现多个TPU之间的协同工作。这种设计特别适合分布式深度学习任务，例如需要多节点并行计算的大规模模型训练。

TPU的体系结构还具有模块化设计，便于扩展以支持更复杂的深度学习任务。谷歌在后续的TPU版本（如TPU v2和v3）中引入了更多的计算核心、更大的内存带宽以及液冷系统，以满足更高性能的需求。这些版本还支持分布式计算架构，可以将多个TPU组装为一个“TPU Pod”，从而实现超大规模的深度学习训练。

==== TPU指令集体系结构

TPU指令集围绕矩阵运算展开，其核心是支持大规模的矩阵乘法与累加（Matrix Multiply-Accumulate, MAC）。矩阵乘法是神经网络中的基础操作，例如卷积层、全连接层等都依赖矩阵乘法进行计算。TPU指令集直接为这些操作提供了硬件支持，通过特定的指令来触发矩阵计算单元（MXU）的运算。这种专用指令避免了传统处理器中复杂的指令解码过程，极大提高了执行效率。

指令集支持低精度数据类型，例如8位整数（INT8）或16位浮点数（bfloat16）。低精度运算是深度学习领域常见的优化手段，用于减少计算开销和存储带宽，同时保证计算精度满足实际需求。TPU指令集为低精度数据提供了原生支持，从硬件层面上提升了性能与能效比。

TPU的指令集采用简化控制逻辑，其设计目标是优化数据流计算，而不是传统处理器那样强调复杂的控制流操作。它通过专用指令控制数据从片外内存到片上存储的传输，以及数据在片上计算单元之间的流动。这种设计符合深度学习任务中数据流计算的特点，显著减少了控制开销。

TPU指令集中的加载与存储指令（Load/Store Instructions）主要用于控制数据的读写操作。TPU片上有统一缓冲区（Unified Buffer）作为高带宽存储，用于缓存神经网络的权重参数和中间计算结果。指令集可以高效管理数据在统一缓冲区和外部存储之间的传输，确保计算单元能够快速获得所需数据。

TPU指令集还包含用于数据预处理与激活函数的指令。例如，指令集中可能会提供用于计算ReLU、Sigmoid、Softmax等常见激活函数的支持，这些操作可以直接在硬件中高效执行，而不需要像传统处理器那样依赖软件实现。

为了支持高并行计算，TPU指令集采用向量化或SIMD（Single Instruction, Multiple Data）方式。一条指令可以同时作用于多个数据元素，从而实现大规模并行计算。这种设计非常适合深度学习中大批量数据的处理需求，能够显著提高计算吞吐量。

TPU指令集的编程接口通常是通过高层次的深度学习框架（如TensorFlow）进行访问的。开发者无需直接编写低级别的指令，而是通过框架中的高阶API描述神经网络模型，框架会自动将这些高层描述转换为TPU的指令集操作。这种设计极大降低了开发者的使用门槛，同时保证了硬件资源的高效利用。

==== TPU微体系结构

TPU微体系结构的核心组件是矩阵乘法单元（MXU，Matrix Multiply Unit），它专门用于高效执行神经网络中的矩阵乘法操作。MXU是一个高度并行的计算单元，能够同时处理大量的乘法累加操作（Multiply-Accumulate, MAC）。在典型的TPU设计中，MXU可以执行以千为单位的MAC操作，从而显著加速神经网络中的卷积计算和全连接层操作。MXU的定制设计使其能够高效利用芯片面积和能量，同时保证低延迟和高吞吐量。

为了支持深度学习模型的参数和中间结果存储，TPU微体系结构包含一个统一缓冲区（Unified Buffer, UB）。统一缓冲区是一个高带宽、低延迟的片上存储，用于缓存权重、激活值和其他中间数据。它与MXU紧密集成，减少了对外部存储的依赖，从而降低了数据传输的延迟和能耗。统一缓冲区的大小经过精心设计，可以容纳神经网络计算中经常访问的数据，避免频繁的数据加载。

TPU微体系结构采用了流水线化设计，将数据流计算划分为多个阶段。每个阶段负责一个特定的任务，例如数据加载、权重解码、矩阵计算和结果写回。流水线设计的优势在于能够同时处理多个任务，从而提高计算单元的利用率和整体吞吐量。此外，流水线还可以缓解数据传输瓶颈，使计算与内存访问的效率更加均衡。

TPU微体系结构使用专用的数据加载和存储单元（Load/Store Units, LSU），负责将数据从片外内存加载到片上统一缓冲区，或者将计算结果写回片外内存。这些单元优化了数据的传输路径和带宽利用率，支持高效的流式计算。通过硬件预取和数据排布优化，LSU可以减少存储访问的延迟，并为计算单元提供持续的高带宽数据流。

TPU微体系结构还集成了激活函数和数据预处理单元，用于执行神经网络中的非线性操作（如ReLU、Sigmoid、Softmax）和数据变换（如归一化、量化等）。这些操作通常是神经网络模型中的重要组成部分，TPU通过硬件加速器为它们提供了高效支持，从而避免了通用处理器中依赖软件实现的开销。

为了支持深度学习任务中的并行性，TPU微体系结构采用大规模并行计算的策略。通过在硬件层面支持SIMD（Single Instruction, Multiple Data）和张量操作，TPU能够同时处理多个数据元素，极大地提升了计算效率。此外，TPU的设计还允许多个核心协同工作，以实现更高的性能和扩展性。

TPU的微体系结构针对深度学习中的低精度计算进行了优化，例如支持bfloat16（16位浮点数）和INT8（8位整数）数据类型。低精度计算可以显著降低数据存储需求和计算开销，同时满足深度学习应用对精度的要求。TPU通过专用硬件实现对低精度运算的支持，进一步提升了计算的能效比。

最后，TPU微体系结构通过专门的硬件控制单元（如调度器和执行单元）实现了对指令流和数据流的高效管理。这些控制单元负责调度计算任务、协调数据传输和管理流水线操作，确保各个硬件模块之间的高效协同工作。

==== TPU实现

TPU的核心实现原理基于矩阵乘法加速。在深度学习中，大量的计算来自于神经网络层的矩阵乘法操作（如全连接层和卷积层的计算）。TPU专门为这些操作设计了一个大规模的矩阵运算单元（Matrix Multiply Unit, MXU），它能够以极高的并行性和效率同时处理数千个乘法累加运算（MAC，Multiply-Accumulate）。这种硬件的高度定制化使得TPU在矩阵运算上的性能远超通用处理器和GPU。

为了实现高效的数据访问和存储，TPU采用了片上存储（On-Chip Memory）和数据流架构（Dataflow Architecture）的组合。TPU的片上存储主要包括统一缓冲区（Unified Buffer），它被用来存储神经网络的权重、输入激活值和中间结果。由于片上存储比片外存储具有更低的访问延迟和更高的带宽，TPU尽可能将经常使用的数据保存在片上缓冲区中，从而减少了片外内存的访问需求。数据流架构通过硬件设计直接管理数据在芯片内部的传输路径，避免了不必要的数据搬移，从而进一步提高了效率。

TPU的实现还通过流水线化设计（Pipelining）来优化数据流和计算流程。TPU将神经网络计算过程分解为多个流水线阶段，每个阶段专注于一个特定的子任务，例如数据加载、权重解码、矩阵运算和结果写回。流水线设计允许多个计算任务同时在不同的阶段中进行，极大地提高了芯片的资源利用率和吞吐量。

在数据表示和计算方面，TPU采用了低精度数据类型优化的原理。与传统的32位浮点运算相比，TPU支持更低精度的计算数据格式，例如bfloat16（16位浮点数）和INT8（8位整数）。这些数据类型能够显著减少计算和存储的资源消耗，同时保持神经网络模型的计算精度。TPU通过硬件支持低精度运算，进一步提升了单位功耗的计算性能（即能效）。

TPU的另一个实现关键是指令流和数据流的高效控制。在传统处理器中，指令流的执行通常需要复杂的控制逻辑，而TPU通过硬件设计简化了控制逻辑，重点优化了数据流。TPU的硬件调度器直接管理数据的加载、存储和运算任务，确保每个硬件单元始终处于高效工作状态。这种优化减少了指令调度和任务切换的开销。

为了支持规模化计算，TPU通过硬件架构实现了大规模并行处理。在硬件层面，TPU设计了大量的并行计算单元，这些单元能够同时处理多个神经网络层的计算任务。在系统级别，TPU还支持多芯片协作，通过多个TPU设备组成计算集群，共同完成大型深度学习模型的训练和推理任务。

TPU的实现还特别注重能效比优化。相比于通用处理器和GPU，TPU通过减少不必要的硬件功能（如分支预测、复杂的缓存层次）以及高度专用化的设计，将更多的芯片资源用于深度学习计算本身，从而显著提高了单位能耗下的计算性能。

最后，TPU的实现原理还依赖于软硬件协同设计。谷歌开发了专门的编译器和框架（如TensorFlow），用于优化神经网络模型在TPU上的运行。这些工具能够将高层的神经网络模型转换为TPU指令，并为硬件执行生成高效的任务调度和数据流图，使TPU硬件的计算潜力得到最大化的发挥。

==== TPU软件

TPU最常用的软件是TensorFlow，它是谷歌开发的一种广泛使用的开源深度学习框架。TensorFlow为TPU提供了深度优化的运行支持，开发者可以通过编写TensorFlow代码无缝地在TPU上运行模型。TensorFlow的TPU支持通过专门的API（如tf.distribute.TPUStrategy）提供分布式训练能力，使用户能够轻松将单机模型扩展到多TPU节点的训练。此外，TensorFlow还包含许多针对TPU的优化功能，例如自动分配计算任务、调整数据流和优化内存管理，使得模型的执行能够充分利用TPU的硬件性能。

为了简化TPU的使用，谷歌还开发了TPU运行时（TPU Runtime），这是一个为TPU设计的底层运行环境。TPU运行时负责管理硬件资源，执行低级硬件指令，以及在TPU硬件和高层编程框架之间提供接口。它确保TensorFlow或其他框架生成的任务能够高效地映射到TPU硬件上，并负责优化任务调度和数据传输。

针对大规模的模型训练和推理任务，谷歌还提供了Cloud TPU，这是一种基于TPU的云服务。Cloud TPU集成了谷歌云平台（Google Cloud Platform, GCP）的功能，用户可以通过GCP访问TPU资源，并通过TensorFlow或JAX运行深度学习模型。Cloud TPU支持自动化的集群管理和任务分发，使得开发者能够快速部署和扩展大规模的深度学习训练任务。

JAX是另一个支持TPU的高性能计算库，它主要用于科学计算和深度学习模型的开发。JAX以其灵活的自动微分和硬件加速能力著称，开发者可以使用JAX轻松实现复杂的数学模型和神经网络架构。在TPU上，JAX通过其后端支持快速矩阵运算和分布式计算，适合需要高度自定义和优化的深度学习任务。

除了TensorFlow和JAX，谷歌还推出了PyTorch/XLA，这是一个为TPU设计的PyTorch扩展。PyTorch/XLA使得开发者可以使用PyTorch框架在TPU上运行模型，同时享受PyTorch的灵活性和动态计算图的特性。PyTorch/XLA将PyTorch代码自动转换为TPU硬件指令，并对模型训练过程进行优化，例如数据分片、梯度计算和模型参数更新。

在工具链方面，谷歌提供了TPU性能分析工具（TPU Profiler），这是一个用于调试和优化TPU程序的性能监控工具。TPU Profiler可以帮助开发者识别性能瓶颈，例如模型的计算、通信或内存使用中的问题，并提供建议以改进模型性能。

此外，为了帮助开发者快速构建和部署深度学习模型，谷歌提供了TensorFlow Model Garden，这是一个集合了许多预训练深度学习模型的库。这些模型经过优化，可以直接运行在TPU上，适合需要快速部署或进行迁移学习的开发者。

最后，为了实现不同模型和数据集的快速训练，谷歌还开发了TPUEstimator和TPU训练循环（TPU Training Loop）。TPUEstimator是一种高级API，它简化了模型定义、训练和评估过程，而TPU训练循环提供了更底层的灵活性，允许开发者精确控制训练步骤、梯度计算和参数更新。

==== 改进TPU

TPU（Tensor Processing Unit）作为谷歌为深度学习工作负载设计的专用硬件，不断在架构、性能、效率和适用性等方面进行改进，以满足深度学习领域快速发展的需求。目前TPU的改进方向主要集中在以下几个方面：

1. 性能提升仍然是TPU改进的核心目标。随着深度学习模型规模的指数级增长（例如GPT系列和更大规模的Transformer模型），TPU需要支持更高的计算能力和吞吐量。未来的TPU改进方向之一是进一步优化矩阵乘法加速器（Matrix Multiply Unit, MMU）的设计，以提升每秒浮点操作次数（FLOPS）。同时，通过更高效的硬件流水线和指令调度机制，减少计算中可能的资源空闲和延迟，从而实现更高效的计算性能。

2. 在存储和内存方面，TPU的改进方向集中于提高片上内存（on-chip memory）的容量和带宽。当前深度学习模型对内存的需求越来越高，大量的模型参数和中间激活值需要快速访问。TPU通过增加SRAM容量、改进内存架构和提升片外存储（如HBM高带宽内存）的访问效率，来满足大规模模型的内存需求。此外，为了应对分布式训练中模型和数据并行的需求，TPU还在优化内存共享和数据缓存技术，以进一步减少数据访问延迟和通信开销。

3. 能效优化是TPU的重要改进方向之一。随着数据中心规模的扩大和深度学习工作负载的增长，降低能耗成为关键问题。TPU通过优化硬件设计、降低计算单元的功耗、改进动态电源管理（Dynamic Power Management）技术，以及采用更先进的制程工艺（如5nm、3nm技术），在提供更高计算性能的同时显著降低能耗。未来TPU还将结合定制化低功耗电路设计和高效的散热技术来进一步提升能效比。

4. 针对深度学习模型多样化需求的硬件支持是TPU的重要改进方向之一。现代深度学习模型不仅包括传统的卷积神经网络（CNN）和全连接网络，还涉及Transformer、图神经网络（GNN）等复杂结构。为了适应这些模型的需求，TPU需要提供更加灵活的硬件支持，例如改进可编程性、支持更多类型的计算模式（如稀疏矩阵计算和动态计算图执行）以及更高效的非矩阵操作（如条件分支和非线性激活函数）。

5. TPU在系统级设计中正在不断优化分布式计算性能，以更好地支持超大规模的训练任务。当前的TPU已经可以通过网络将多个TPU芯片组成一个集群（如TPU Pod），支持数千个TPU核心同时工作。未来，TPU将改进芯片间的互连带宽和延迟，采用更高效的通信协议（如RDMA），并优化全局同步机制，以降低集群规模增加带来的通信开销。特别是在支持混合精度计算和梯度压缩技术方面，TPU将进一步优化，以适应更高效的分布式深度学习训练。

6. TPU在软件生态系统的改进方向上也在持续投入。为支持更广泛的开发者群体，TPU将进一步优化编程接口（如TensorFlow、JAX、PyTorch/XLA等）和自动化工具链，使其使用更加便捷。自动混合精度训练、模型并行策略优化以及分布式训练自动化等功能的改进，将帮助开发者更轻松地利用TPU的计算能力。

7. TPU还将改进对推理任务的支持。目前TPU主要被用于训练阶段的高计算需求，但推理任务由于对延迟和吞吐量的不同需求，也成为TPU改进的重点之一。未来TPU将在低延迟推理和边缘设备部署方面进行优化，例如设计针对推理的定制加速器，以及支持模型剪枝、量化和稀疏计算等推理优化技术。

=== Microsoft Catapult——一种灵活的数据中心加速器

==== Catapult实现与体系结构

Microsoft Catapult 是微软为提高其数据中心计算性能和效率而设计的一种硬件加速平台。Catapult 项目采用 FPGA（现场可编程门阵列）作为加速硬件，通过灵活的硬件设计与定制化优化，为多种任务（尤其是搜索引擎、机器学习等计算密集型应用）提供加速能力。

Catapult 的实现基于将 FPGA 集成到数据中心的每个服务器中，通过 FPGA 和 CPU 的协同工作，为高性能计算任务提供硬件加速。Catapult 的设计目标是灵活性和高效性，它不仅能够在不同的任务需求下动态配置硬件资源，还能显著降低数据中心的整体功耗。

在体系结构方面，Catapult 的核心由以下几个部分组成：

1. FPGA 节点设计：Catapult 在每台服务器中嵌入一块 FPGA，作为与 CPU 协作的硬件加速器。这些 FPGA 通过高速接口（如 PCIe）与主机 CPU 连接，并通过标准协议实现数据交换。在设计中，FPGA 执行计算密集型任务，例如搜索排序算法、深度学习推理或数据流处理，而 CPU 负责控制逻辑和任务调度。

2. 分布式 FPGA 网络：Catapult 通过高带宽网络将多个 FPGA 节点连接成一个分布式加速系统。该网络支持 FPGA 间的低延迟通信，使得数据中心的 FPGA 加速器能够协同工作。这种设计特别适用于需要并行处理的大规模任务，例如搜索引擎结果排序或大规模机器学习模型的推理计算。

3. 模块化设计：Catapult 的 FPGA 配置为模块化结构，每个模块执行特定功能（例如特定算法或数据流处理）。这种模块化设计使得系统能够根据不同任务需求动态加载不同的 FPGA 配置文件（bitstream），从而提高灵活性和资源利用率。

4. 流水线并行计算：Catapult 的 FPGA 被设计为支持流水线并行计算。这种计算模式将任务划分为多个阶段，每个阶段由 FPGA 的特定硬件资源处理，从而实现更高的吞吐量和更低的延迟。这种设计非常适合搜索排序或大规模数据流处理等任务。

5. 硬件与软件协同优化：Catapult 在设计中重视硬件和软件的协同优化。微软为 FPGA 开发了定制化的软件栈，包括硬件描述语言（HDL）开发工具链和支持任务调度的高层编程接口。开发者可以通过这些工具方便地在 FPGA 上部署算法，从而降低开发复杂性。

Catapult 的实现原理是通过 FPGA 的可编程特性，为特定任务设计高效的硬件加速器。与传统的 GPU 或 ASIC 加速器不同，FPGA 的优势在于其灵活性和低延迟特性。Catapult 的体系结构充分利用了 FPGA 的这些特点，将其与数据中心的服务器整合，从而实现了硬件资源的统一调度与高效利用。

Catapult 的一个关键应用是 Bing 搜索引擎的加速。通过 FPGA，微软实现了排序算法的硬件加速，大幅度提高了搜索结果的处理速度。此外，Catapult 还被用作深度学习推理的加速器，用于优化计算密集型神经网络任务。

==== Catapult软件

Microsoft Catapult 的成功不仅依赖于其硬件设计，还依赖于其开发的软件栈和相关工具，这些软件为开发者提供了高效的硬件配置、任务管理和应用部署能力，充分释放了 FPGA 硬件的潜能。以下是 Catapult 平台的重要软件及其作用的详细介绍。

Catapult 软件栈涵盖从底层硬件抽象到高层应用接口的一整套工具。它的设计目标是简化 FPGA 的编程流程，提高开发效率，并使其能够与数据中心的现有工作流和服务无缝集成。Catapult 的软件系统主要包括以下几个重要部分：

1. 硬件描述与配置工具  
+

Catapult 使用标准的硬件描述语言（如 Verilog 和 VHDL）来设计 FPGA 的硬件逻辑电路。同时，微软开发了定制化的工具链，以便简化 FPGA 的硬件描述和编译流程。这些工具能够将高层的任务描述转化为 FPGA 的硬件配置文件（bitstream），并快速部署到 Catapult 平台中的 FPGA 节点上。

2. 高层次综合工具（HLS, High-Level Synthesis）  
+

为了降低 FPGA 开发的复杂性，Catapult 支持高层次综合工具，使开发者可以使用高层次编程语言（如 C 或 C++）来描述任务逻辑。HLS 工具会将这些描述自动转换为底层的硬件逻辑，从而生成适用于 FPGA 的硬件电路。这大大降低了开发门槛，使得非硬件领域的开发者也能够高效利用 FPGA 的计算能力。

3. FPGA 任务调度与资源管理软件  
+

Catapult 平台包含任务调度和资源管理软件，用于管理 FPGA 与服务器 CPU 的协同工作。这些软件负责在不同任务间分配硬件资源，并动态调整 FPGA 的配置以适应任务需求。例如，在 Bing 搜索引擎加速任务中，任务调度器可以根据查询流量动态加载不同的排序算法，从而提高服务效率。

4. 分布式系统支持工具  
+

Catapult 平台的 FPGA 加速器通过高速网络连接形成分布式计算系统。为了管理这些 FPGA 节点之间的通信，微软开发了分布式系统支持工具。这些工具能够在多个 FPGA 节点间协调任务分配，优化通信路径，并实现低延迟的数据交换。这种分布式支持对需要高并行性和高吞吐量的应用（如机器学习推理）至关重要。

5. Catapult 驱动程序与运行时库  
+

Catapult 的驱动程序和运行时库负责连接 FPGA 和主机 CPU，并提供与应用程序交互的接口。开发者可以通过标准化的 API（如微软提供的 DNN 模型推理 API）调用 FPGA 的加速功能，而无需直接处理底层硬件逻辑。这种抽象化设计简化了开发流程，同时保证了应用的可移植性。

6. 机器学习支持工具  
+

在机器学习领域，Catapult 平台提供了针对深度学习推理的优化工具。这些工具支持常见的深度学习框架（如 TensorFlow 和 PyTorch），并提供与 FPGA 加速紧密集成的库和优化算子。例如，Catapult 提供了硬件优化版本的矩阵乘法和卷积操作，以显著提升推理速度。

7. 开发与调试工具链  
+

Catapult 的开发工具链包含硬件调试、性能分析和故障诊断工具。这些工具能够帮助开发者检测 FPGA 配置中的错误，分析硬件加速性能瓶颈，并对其进行优化。例如，微软为 Catapult 平台开发了图形化调试界面，使开发者可以直观地监控硬件运行状态。

8. FPGA 动态重新配置支持  
+

Catapult 的软件栈支持 FPGA 的动态部分重配置（Partial Reconfiguration），即在不中断 FPGA 其他部分任务运行的情况下，重新加载某些模块的硬件配置。这种特性使得 Catapult 平台能够灵活应对不同的任务需求，提高了硬件资源的利用效率。

==== Catapult上的CNN

Microsoft Catapult 平台上的 CNN（卷积神经网络）实现，充分利用了 FPGA（现场可编程门阵列）硬件的灵活性和高效并行性，为深度学习推理任务（如图像分类、目标检测等）提供了强大的加速能力。

在 Catapult 上运行 CNN 的核心思想是利用 FPGA 的并行计算特性加速 CNN 的关键运算，尤其是卷积运算和全连接层运算。这些操作占据了 CNN 绝大部分的计算开销，同时也具有高度的计算密集性和数据重复利用性。Catapult 针对这些特点，通过硬件优化和高效的软件栈支持，显著提高了 CNN 的推理性能。

Catapult 平台上 CNN 的主要特性和实现方法如下：

1. 卷积层加速  
+
 
卷积层是 CNN 的核心计算模块，涉及大量的矩阵-张量乘法操作。Catapult 平台在 FPGA 上实现了专门的卷积硬件模块，这些模块利用数据的高局部性和重复性，将卷积操作映射为并行的乘加运算单元。通过流水线技术和硬件级优化，卷积模块可以同时处理多个输入通道和输出通道的数据流，从而极大地提升计算效率。

2. 稀疏性和低精度优化  
+

在 CNN 推理过程中，可以通过剪枝、量化等技术减少模型参数的稀疏性和数据的位宽需求。Catapult 平台支持 8 位甚至更低精度的整数计算，以及对稀疏矩阵的优化处理。通过这些优化，Catapult 在不显著损失精度的前提下减少了计算量和存储带宽需求，同时更好地利用了 FPGA 的硬件资源。

3. 特定层的硬件优化 
+

除了卷积层外，Catapult 平台还针对全连接层、池化层和激活函数实现了硬件优化。全连接层的计算被映射为高效的矩阵乘法，并利用 FPGA 的片上存储器来减少访存开销。池化操作（如最大池化或平均池化）被设计为独立的硬件单元，与卷积层流水线连接以最大化吞吐量。激活函数（如 ReLU 或 sigmoid）则通过查找表和硬件算子快速实现。

4. 深度学习框架的集成支持  
+

Catapult 平台与主流深度学习框架（如 TensorFlow 和 PyTorch）深度集成，开发者可以直接通过这些框架生成 CNN 模型，并利用 Catapult 的工具链将模型转换为适用于 FPGA 的硬件配置文件。这种高层次的抽象简化了开发过程，使得开发者无需直接接触 FPGA 的底层硬件逻辑。

5. 分布式 CNN 推理支持  
+

Catapult 平台通过其分布式计算能力支持大型 CNN 模型的分片推理。当单个 FPGA 无法容纳整个 CNN 模型时，Catapult 能够将模型划分为多个部分并分配到不同的 FPGA 节点上运行。通过高速互连网络，Catapult 能够高效地在节点之间传输中间数据，保持推理过程的低延迟和高吞吐量。

6. 动态重新配置 CNN 模型 
+

Catapult 的 FPGA 硬件支持动态部分重配置（Partial Reconfiguration），允许在不中断硬件运行的情况下加载新的 CNN 模型配置。这种特性使得 Catapult 平台能够灵活应对不同的 CNN 模型需求，如不同任务间的切换或模型参数的在线调整。

7. 典型应用  
+

在图像处理领域，Catapult 上的 CNN 被广泛用于图像分类、目标检测和语义分割等任务。例如，微软的 Bing 搜索引擎使用 Catapult 平台加速图片的特征提取和分类。此外，在视频处理和流媒体分析等任务中，Catapult 平台也通过 CNN 模型实现了高效的实时处理。

通过以上优化，Microsoft Catapult 平台不仅大幅提升了 CNN 模型的推理性能，还提供了高效的开发环境和分布式支持，满足了数据中心中对低延迟、高吞吐量和灵活性的需求。在 AI 加速领域，Catapult 平台的 CNN 实现成为一种高效且具有成本效益的解决方案。

==== Catapult上的搜索加速

Microsoft Catapult 是微软的一项基于 FPGA 的加速平台，最初被设计用于 Bing 搜索引擎的加速任务。通过将搜索引擎的关键计算任务卸载到 FPGA 上，Catapult 平台大幅提升了搜索引擎的性能、效率和响应速度。

Catapult 平台上的搜索加速主要体现在以下几个方面：

首先，Catapult 平台通过 FPGA 的并行处理能力加速了搜索引擎中的关键任务，如查询解析、文档评分、排序以及推荐等。在传统 CPU 中，这些任务需要消耗大量计算资源并具有较高的延迟。而通过 FPGA 的硬件流水线和高并行度特性，Catapult 平台能够快速处理大规模的搜索任务，从而显著降低延迟并提高吞吐量。

其次，Catapult 平台专门优化了 Bing 搜索引擎的文档评分过程。在搜索任务中，用户输入的查询需要与数十亿文档进行匹配，并计算相关性得分以返回最相关的结果。文档评分过程通常涉及复杂的向量操作、字符串匹配和特征提取等计算。Catapult 使用 FPGA 加速了这些运算，通过硬件级的优化实现高效的矩阵-向量乘法、稀疏向量操作和哈希匹配等功能，从而大幅提升了文档评分的速度。

此外，Catapult 平台还支持大规模分布式搜索加速架构。在 Bing 的数据中心中，数千个 FPGA 组成一个高性能的加速网络，每个 FPGA 节点负责处理搜索任务的一部分。这种架构允许任务在不同节点之间高效分配和并行执行，并通过高速互联网络实现节点间的数据通信，从而实现搜索任务的低延迟和高吞吐量。

在搜索排序任务中，Catapult 平台也起到了至关重要的作用。在搜索引擎中，排序算法根据查询结果的相关性对文档进行优先级排序。Catapult 平台使用 FPGA 加速了排序过程中的特征计算和评分函数评估，并利用硬件流水线进一步优化排序的执行效率。这种硬件加速确保了用户能够以更快的速度获得更准确的搜索结果。

Catapult 平台还支持深度学习模型在搜索引擎中的部署。近年来，深度学习模型在搜索相关性评估、自然语言处理和推荐系统中得到了广泛应用。Catapult 利用 FPGA 的灵活性和高并行性，为这些深度学习模型提供了硬件加速支持。通过优化的硬件配置，Catapult 平台能够加速深度学习模型的推理过程，例如在查询理解和上下文匹配任务中显著提升性能。

最后，Catapult 平台的设计不仅提升了 Bing 搜索的性能，还降低了整体能耗和硬件成本。相比使用更多的 CPU 或 GPU 资源，FPGA 提供了更高的性能功耗比，同时可以通过硬件复用支持多种搜索任务的加速需求。Catapult 平台的灵活性还使其能够适应搜索引擎的不断发展，例如支持新算法、新模型和更复杂的任务。

综上所述，Microsoft Catapult 平台在搜索加速中的应用充分利用了 FPGA 的硬件优势，通过大规模并行处理、硬件流水线优化和深度学习模型支持，大幅提升了搜索引擎的性能和响应效率。同时，其分布式架构和灵活性使其成为 Bing 搜索引擎的重要支撑技术，也是现代数据中心搜索加速的典范之一。

==== Catapult Ver 1 的部署

Catapult Ver 1 的部署始于 Microsoft Bing 数据中心。为了应对搜索引擎对计算性能的巨大需求以及降低搜索延迟的挑战，微软决定通过 FPGA 提供硬件加速。与传统 CPU 或 GPU 不同，FPGA 提供了灵活的硬件配置能力和低延迟的并行处理特性，非常适合搜索引擎中的关键任务加速。

Catapult Ver 1 的部署主要体现在以下几个方面：

首先是硬件部署。在 Catapult Ver 1 中，每台服务器中都集成了一个专门的 FPGA 加速卡。这些 FPGA 加速卡通过 PCIe 总线与主机 CPU 相连，形成了一个计算协同的架构。每个 FPGA 配备有独立的片上内存，用于处理本地数据，同时支持通过网络与其他 FPGA 节点进行通信。在 Bing 数据中心的部署过程中，微软为数千台服务器安装了这些 FPGA 加速卡，使其能够支持大规模的分布式计算任务。

其次是软件堆栈部署。Catapult Ver 1 的软件栈被设计为一个分层结构，包括驱动程序、运行时环境和应用程序接口（API）。这些软件层使得主机 CPU 和 FPGA 之间能够高效协同工作。在实际部署中，微软开发了一套专门的 FPGA 配置工具，用于将搜索引擎的关键任务逻辑编译成 FPGA 的硬件描述语言（HDL），并加载到 FPGA 中运行。这种硬件逻辑优化后的 FPGA 能够以较低的能耗完成搜索任务中的复杂计算。

网络部署是 Catapult Ver 1 中的另一大亮点。微软通过高性能网络将部署在服务器中的 FPGA 节点连接起来，形成一个可扩展的分布式加速网络。在这个网络中，FPGA 节点之间可以直接通信，实现了搜索任务的并行处理。通过这种分布式架构，Catapult Ver 1 能够支持 Bing 数据中心中的全局负载均衡和任务分配，从而进一步提升了系统的效率。

Catapult Ver 1 的部署还涉及到具体的搜索引擎加速任务。在 Bing 搜索中，查询处理涉及大量计算密集型任务，例如查询解析、文档评分和排序等。微软将这些任务的核心逻辑迁移到 FPGA 中运行，而非完全依赖于传统的 CPU 计算。这种硬件加速方式有效降低了搜索查询的响应时间，同时提升了搜索结果的相关性和准确性。

此外，Catapult Ver 1 的部署过程还注重成本和效率的平衡。与大规模升级服务器 CPU 不同，部署 FPGA 加速卡为微软节约了大量成本。FPGA 的低功耗和高性能功耗比使得数据中心能够在提高性能的同时保持能源消耗的稳定。在 Catapult Ver 1 项目的后期部署中，微软的数据中心还通过动态调整 FPGA 配置来满足不同工作负载的需求，这进一步增强了部署的灵活性。

==== Catapult Ver 2

Microsoft Catapult Ver 2 是微软在第一代 FPGA 加速平台 Catapult Ver 1 的基础上开发的升级版本，旨在进一步优化数据中心的硬件加速能力，并扩展应用领域。Catapult Ver 2 的设计目标是不仅支持 Bing 搜索引擎的加速，还能够为其他计算密集型任务（例如深度学习推理、网络功能虚拟化等）提供通用硬件加速。这一版本在体系结构、功能支持和部署模式上都进行了显著的改进。

Catapult Ver 2 的体系结构以 灵活性和扩展性 为核心特性。与 Ver 1 相比，Ver 2 将 FPGA 节点的连接方式从单独通过 PCIe 接口与主机 CPU 相连，扩展为一个更加复杂的分布式架构。这种改进使得 FPGA 不仅可以加速本地服务器的任务，还能够通过网络协作完成大规模的分布式任务处理。Catapult Ver 2 中，FPGA 加速卡通过高速网络接口连接在一起，形成了一个可扩展的计算加速池，进一步提升了系统的整体吞吐量。

在硬件配置上，Catapult Ver 2 引入了更强大的 FPGA 芯片，同时在加速卡上集成了更多的片上存储器和更高带宽的接口。这种硬件升级使得 FPGA 能够以更高的性能运行复杂的计算任务，例如深度学习模型的推理和训练。Catapult Ver 2 的硬件设计还加入了对 高精度计算 和 低精度运算 的支持，从而为不同类型的任务提供优化选项。

在软件层面，Catapult Ver 2 提供了一套更加完善的软件堆栈，支持多种工作负载和开发环境。这一版本的 Catapult 软件堆栈包括驱动程序、运行时库和开发工具链，开发者可以利用标准的编程语言（例如 C++ 和高层次综合工具）编写加速代码。微软还为 Catapult Ver 2 引入了深度学习框架支持，例如与 CNTK 和 TensorFlow 的无缝集成，使得用户能够轻松部署神经网络模型到 FPGA 上运行。此外，Catapult Ver 2 的软件支持动态重配置，允许开发者根据工作负载的需求动态更改 FPGA 的功能逻辑，从而实现资源利用的最大化。

Catapult Ver 2 的部署不仅延续了 Ver 1 在 Bing 搜索中的应用，还大幅扩展到其他领域。例如，微软开始在其 Azure 云服务中使用 Catapult Ver 2 提供加速功能，用于支持虚拟机的网络流量管理、视频转码和分布式数据库查询加速。这些新增的应用场景要求 FPGA 的灵活性和吞吐能力更高，而 Ver 2 的改进设计正好满足了这些需求。

一个关键的改进是，Catapult Ver 2 在网络拓扑上采用了近端加速器和全局共享资源池的混合架构。通过将部分 FPGA 加速卡设置为共享资源池中的节点，Catapult Ver 2 实现了更高的计算资源复用率和任务分配效率。这种设计特别适合需要并行处理的任务，例如搜索引擎的大规模查询解析和排序计算，以及深度学习推理中的批量推理处理。

此外，Catapult Ver 2 针对低延迟任务优化了通信机制。FPGA 加速器之间通过高速网络直接交换数据，而无需通过主机 CPU 中转。这种直接通信机制大幅降低了延迟，同时减少了主机 CPU 的负担，提高了整体系统的响应速度。

Catapult Ver 2 的一个重要应用是 深度学习推理加速。与传统的 GPU 加速相比，FPGA 在执行特定任务（例如低精度矩阵乘法）时具有更高的能效比。微软利用 Catapult Ver 2 提供的高效硬件加速功能，在 Azure 数据中心中运行深度学习模型推理，并显著降低了功耗和运营成本。

=== Intel Crest——一种用于训练的数据中心加速器

Intel Crest 是 Intel 公司推出的一种数据中心专用加速器 (Data Center ASIC)，旨在为特定的数据密集型计算任务提供高效的硬件加速。Crest 主要应用于云计算和数据中心的高性能计算场景，特别是在数据分析、机器学习推理以及高吞吐量任务中表现突出。

Crest 的设计目标是为数据中心工作负载提供更高的性能、能效比和计算密度，与传统的 CPU 或 GPU 相比，针对性更强，更适合某些固定的计算模式。作为 ASIC (Application-Specific Integrated Circuit) 硬件加速器，Crest 专为高效处理特定计算任务设计，具有比通用硬件（如 CPU 或 GPU）更低的功耗、更高的性能和更好的延迟表现。

==== Crest 的特点

1. 高性能定制设计
+

Crest 是专用的 ASIC 加速器，因此其设计完全针对数据中心中常见的特定工作负载进行了优化。它能够以更高的吞吐量和更低的功耗执行计算密集型任务，例如深度学习推理、视频处理或大规模数据分析。

2. 灵活性与可扩展性
+

Crest 被设计为数据中心架构的一个组成部分，支持多种并行计算模型（例如 SIMD 和 MIMD）。虽然 ASIC 硬件通常比 FPGA 更固定，但 Crest 针对不同工作负载仍具有一定的灵活性，能够适应多种任务需求。

3. 片上存储器与高带宽
+

Crest 采用了片上存储器（on-chip memory）的设计，为处理单元提供了高速数据访问路径，减少了与外部存储器通信的延迟，同时增加了数据访问带宽。这种设计特别适合需要高内存带宽的工作负载，如矩阵运算和图形处理。

4. 低功耗高能效
+

作为 ASIC 的一部分，Crest 的能效远高于通用硬件（例如 GPU），因为其硬件逻辑专门为特定任务设计，省去了大量通用硬件中必需的多余功能。这种高效设计使 Crest 在数据中心部署中具有显著的功耗优势。

5. 多种数据格式支持 
+

Crest 能够处理包括浮点数和定点数在内的多种数据格式。这种支持让它在深度学习推理任务中表现出色，因为低精度运算（如 8 位、16 位整数计算）在现代 AI 推理任务中非常常见。

==== Crest 的用途

1. 深度学习推理加速
+

Crest 在深度学习推理中表现突出。它能够高效地处理神经网络的前向传播，尤其是在处理卷积神经网络 (CNN) 等计算密集型模型时，通过高效的矩阵运算加速推理过程。

2. 视频处理与编码
+

视频流的实时处理是数据中心的常见任务。Crest 可以通过高带宽和高并行性支持视频流的实时解码、编码和转码，为在线视频平台提供支持。

3. 大规模数据分析
+

数据中心中经常需要执行大规模数据分析工作负载，例如排序、过滤和聚合等操作。Crest 的高吞吐量设计使其在处理这些任务时具有显著的性能优势。

4. 图形数据与科学计算
+

Crest 还可以应用于图形数据处理和科学计算，特别是在需要进行复杂数学运算或矩阵操作的工作负载中。

==== Crest 的部署场景

Intel Crest 主要部署在数据中心环境中，用于加速大规模工作负载。它可以与传统 CPU 协同工作，将计算密集型任务卸载到 Crest，以减轻 CPU 的负担。与 GPU 或 FPGA 相比，Crest 提供了更高效的能耗比和更优化的性能，特别适合固定功能需求的任务。

=== Pixel Visual Core——一种个人移动设备图像处理单元

Pixel Visual Core（PVC）是谷歌公司为其智能手机系列（例如 Pixel 手机）设计的一种专用图像处理单元（Image Processing Unit，IPU）。PVC 的开发旨在提升图像处理的性能，同时降低功耗，以支持复杂的计算摄影任务和实时图像处理。PVC 是一种高度优化的硬件加速器，能够实现高效的图像处理操作，包括 HDR（高动态范围）图像生成、降噪、白平衡调整以及其他先进的图像处理功能。

PVC 集成了多个硬件加速器和自定义的处理核心，专门优化计算摄影和机器学习任务。它能够以硬件为基础实现诸如 HDR+ 和夜视模式等复杂功能，这些功能原本需要消耗大量 CPU 和 GPU 资源。在谷歌 Pixel 系列手机中，PVC 的引入显著增强了拍照体验，使用户能够在极短时间内获得高质量的图像，并在低光环境下实现细节丰富的画面。

PVC 的体系结构设计强调并行计算能力。它包含多个专用处理核心，能够并行处理图像的不同区域，从而极大地提高了数据吞吐量。此外，PVC 支持广泛的图像处理算法，并且通过硬件优化实现了更高效的能耗比，与依赖 CPU 和 GPU 的传统图像处理方法相比，它在降低功耗的同时大幅提高了处理速度。

PVC 的一个显著特点是其灵活性和可编程性，它支持包括 TensorFlow Lite 在内的机器学习框架，使得开发者可以针对 PVC 开发定制化的图像处理算法。这一特性不仅支持先进的计算摄影功能，还为后续的软件升级和新功能实现提供了支持。

Pixel Visual Core 的用途广泛，除了拍照功能的提升，它还被用于增强视频录制、实时滤镜和增强现实（AR）应用的性能。例如，在拍照时，PVC 可以实时分析和处理多帧图像，从而生成更清晰、色彩更准确的 HDR+ 照片。在视频录制中，它能够实时应用降噪和动态范围扩展，显著提升画面质量。同时，由于其支持机器学习算法，PVC 在语义分割、人脸识别等计算密集型任务中也有广泛应用。

==== Pixel Visual Core 体系结构的理念

Pixel Visual Core（PVC）的体系结构设计理念基于高性能、低功耗和专用性优化的原则，专注于满足计算摄影和实时图像处理的需求，同时为机器学习任务提供强大的硬件支持。其设计理念体现了以下核心思想：

1. PVC 的体系结构设计强调并行计算能力。现代计算摄影和图像处理通常需要处理大规模的图像数据，传统的串行处理方法无法满足实时性能要求。PVC 的设计理念是通过高度并行化的硬件结构，将图像分割为多个小块，分发到不同的计算单元并行处理，从而实现大幅度的性能提升。其硬件设计包含多个专用核心，这些核心可以同时运行，以快速完成复杂的图像计算任务。

2. PVC 的设计注重能效比优化。在移动设备中，功耗限制是一个关键问题，尤其是在处理高计算量的任务（例如 HDR+、降噪和白平衡）时。PVC 的架构通过硬件加速和专用指令集减少了对通用 CPU 和 GPU 的依赖，降低了整体功耗，同时提升了执行速度。这种设计使得 PVC 能够在低功耗的同时支持复杂的计算任务，延长设备的续航时间。

3. PVC 强调硬件的灵活性和可编程性。虽然 PVC 是一个专用硬件，但其体系结构支持广泛的算法和机器学习框架，例如 TensorFlow Lite。这种设计允许开发者为 PVC 编写特定的软件算法，使其能够快速适应新兴的图像处理需求，例如更高效的 HDR、语义分割或 AR 应用。PVC 的灵活性使其不仅限于静态图像处理，还可以扩展到视频处理和实时 AI 推理。

4. PVC 的架构理念还包含模块化和集成性。它并不是完全独立的硬件，而是深度集成在设备的 SoC（系统级芯片）中，能够与 CPU、GPU 和内存系统高效协同工作。这种集成性减少了数据传输的延迟和带宽消耗，使得 PVC 可以高效地与其他硬件单元交互，支持实时任务处理。

5. PVC 的设计体现了专用性和通用性的平衡。作为一个专用图像处理单元，PVC 针对计算摄影和 AI 任务进行了高度优化，但其设计并不局限于特定的算法或任务，而是为未来的计算需求预留了足够的扩展空间。这种设计理念确保了 PVC 能够应对图像处理领域不断变化的需求，同时为设备提供前沿的图像处理能力。

==== Pixel Visual Core 光晕

当模块计算滑动窗口到达一个二维数据的边缘部分时，3 × 3, 5 × 5, 7 × 7模板分别要从边缘外取1、2、3个像素（模板维度的一半减一）。这给出了两种选择。Pixel Visual Core要么无法充分利用边界附近元素对应的硬件资源（因为它们只传递输入值），要么使用去掉了ALU的简化版PE对二维PE稍做扩展。因为标准 PE与简化版 PE的大小相差约2.2倍，所以 Pixel VisualCore选择了后者。这个扩充区域称为光晕（halo）。

==== Pixel Visual Core 的处理器

7.7.5 Pixel Visual Core的处理器l6×l6个PE和每个维度上的4个光晕通道，合在一起称为PE阵列（PEarray）或向量阵列(vector array)，它是Pixel VisualCore的主要计算单元。它还有一个加载一存储单元，称为片生成器(sheet generator，SHG)。SHG是指对大小为1 × l到256 × 256的像素块进行的存储器访问，这样的像素块被称为片（sheet）。这种访问发生在下采样时，典型值为16 × 16或20 × 20。

Pixel Visual Core的实现可以拥有任意偶数个核，具体取决于可用资源。因此，它需要一个网络将这些核连接在一起，所以每个核还有一个接口连到片上网络（network on chip，NOC）。但是，Pixel Visual Core的典型NOC实现不会是一个昂贵的交叉开关，因为这需要数据通过很长的距离，而这样做的成本非常高。利用此应用程序的流水线本质，NOC通常只需要与相邻的核通信。它被实现为一种二维网格，由软件管理核的电源门控。

最后，Pixel Visual Core 还包含一个标量处理器，称为标量通道（scalar lane,SCL）。它与向量通道相同，只是增加了一些处理跳转、分支和中断的指令，控制到向量阵列的指令流，并为片生成器调度所有加载和存储操作。它还有一个很小的指令存储器。注意，Pixel Visual Core使用一个控制标量单元和向量单元的单一指令流，类似于一个CPU核为其标量和SIMD单元使用单一指令流。

除了核之外，还有一个DMA引擎用于在DRAM和行缓冲区之间传送数据，同时高效地在图像存储布局格式（即压缩/解压缩）之间进行转换。与顺序DRAM访问一样，这些DMA引擎也执行与向量类似的DRAM集中读操作，以及顺序和步幅读写。

==== Pixel Visual Core 指令集体系结构

与GPU类似，Pixel Visual Core采用一种两步编译过程。第一步是将程序由原语言（例如，Halide）编译为vISA指令。Pixel Visual Core vISA（virtual instruction set architecture,虚拟指令集体系结构）受到了RISC-V指令集的启发，但它采用了一种图像专用的存储模型，并对指令集进行了扩展以进行图像处理，特别是图像的二维概念。在vISA中，一个核的二维阵列是无限的，寄存器的数量是无限的，存储器大小也没有限制。vISA指令包含了不直接访问DRAM的纯函数，极大地简化了将其映射到硬件的操作。

第二步是将vISA程序编译为pISA（物理指令集体系结构）程序。以vISA为编译器的目标，处理器可以与之前的程序保持软件兼容，同时还能接受对pISA指令集的修改，所以vISA扮演的角色类似于PTX在GPU中扮演的角色。由vISA降至pISA分为两步：编译以及与早期绑定参数的映射，向代码中添加后期绑定参数。必须绑定的参数包括STP大小、光晕大小、STP的数量、行缓冲区的映射，将内核映射到处理器，以及寄存器和局部存储器分配。

pISA是一个超长指令字（VLIV）指令集，拥有宽度为119位的指令。第一个字段的长度为43位，用于标量通道；第二个字段的长度为38位，指定由二维PE阵列执行的计算：第三个字段的长度为12位，指定由二维PE阵列执行的存储器访问；最后两个字段是用于计算或寻址的立即数。所有VLIV字段的操作是我们所期望的：二进制补码整数算术运算、饱和整数算术运算、逻辑运算、移位、数据传输，以及一些特殊运算，比如除法选代和计算前导零的个数等。标量通道在二维PE阵列中支持这些运算的一个超集，另外还增加了用于控制流和片生成器控制的指令。上面提到的1位谓词寄存器支持向寄存器的条件移动（例如，若C，则A=B）。

尽管pISA VLIW指令非常宽，但Halide内核很短，经常仅有200-600条指令。作为一个IPU，它只需要执行一个应用程序中计算密集的部分，而将其他功能交给CPU和GPO因此，Pixel Visunl Core的指令存储器仅保存2048条pISA指令（28.5KiB）。

标量通道发出访问行缓冲区的片生成器指令。与Pixel Visual Core中的其他存储器访问不同，其延迟可能超过1个时钟周期，所以它们有一个类似于DMA的接口。使用这个通道需首先在特殊功能寄存器中设定地址和传送大小。

==== Pixel Visual Core PE

体系结构设计中的一个决策就是设定光晕的大小。Pixel Visual Core使用16 × 16个PE，并增加了一个拥有2个额外单元的光晕，所以它可以直接支持5 × 5模板。注意，PE阵列越大，支持给定模型大小所需要的光晕相对开销越小。

对于Pixel Visual Core，光晕 PE的较小尺寸和16 × l6的阵列规模意味着光晕只需要多占用20%的面积。对于一个5 × 5的模板，Pixel Visual Core每个时钟周期可以计算约1.8倍（stem:[s6^2/12^2]）的结果；对于3×3模板，此比值约为1.3(stem:[16^2/14^2])。

PE的算术运算单元的设计受乘累加（MAC）运算的驱动，这种运算是模板计算的基元运算。Pisel Visual Core 原生MAC的乘法宽度为16位，但它们能够以32位宽度进行计算。MAC的流水化设计会不必要地耗用能量，这是因为要对所增加的流水线寄存器进行读写操作。因此，来加硬件的耗时就决定了时钟周期。之前提到的其他一些运算是传统的逻辑与算术运算，还有算术运算的饱和版本及一些专用指令。PE有两个16位ALU，它们可以在单个时钟周期内以各种方式运行。

* 独立，生成两个16位结果：A op C, C op D

* 融合，仅生成一个16位结果：A op (C op D)

* 联合，生成一个32位结果：A:C op B:D

==== 二维行缓冲区及其控制器

因为DRAM 访问耗用如此之多的能量，所以要对Pixel Visual Core存储系统进行精心设计，使DRAM访问的次数降至最低。这里的关键创新是二维行缓冲区。

逻辑上，内核运行在独立的核上，它们连接在一个DAG中，输入来自传感器或DRAM，编出送至DRAM。行缓冲区在内核之间保存要计算的图像的一部分。

二维行级冲区必须支持如下4项功能。

1. 它必须支持各种大小的二维模板计算，而这些大小在设计时是未知的。

2. 由于光晕的原因，对于Pixel Visual Core中的16x16 PE阵列，STP将希望从行缓冲区中读取20×20的像素块，向行缓冲区写人16×16的像素块。

3. 因为DAG是可编程的，所以我们需要可以由软件在任意两个核之间分配的行缓冲区。

4. 几个核可能需要从同一个行缓冲区读取数据。因此，一个行级冲区应当支持多个消费者，尽管它只需要一个生产者。

Piel Visual Core中的行缓冲区实际上是一个多读取者的二维FIFO抽象，建立在大量SRAM之上：每个实例中为128KiB。它包含了仅使用一次的临时“图像”，对于这些图像，一个小型专用本地FIFO的效率要远高于对远距离存储器中的数据进行缓存。

由于读取的是20 × 20的像素块，而写入的是16 × 16的像素块，所以为了适应这一大小失配，FIFO中的基本分配单元为4×4的像素组。每个模板处理器有一个行缓冲区池(lincbuferpool，LBP），它们可能拥有8个逻辑行缓冲区（LB），再加上一个用于I/ODMA的LBP。LBP有三级抽象。

1. 在顶端，LBP控制器支持将8个LB作为逻辑实例。每个LB有一个FIFO生产者，最多有8个FIFO消费者。

2. 控制器跟踪每个FIFO的头指针和尾指针。注意，LBP内部行缓冲区的大小灵活可变，由控制器决定。

3. 底部是多个物理存储体，用于提供所需的带宽。Pixel Visual Core拥有8个物理存储体，每个存储体有一个128位的接口，容量为l6KiB。LBP的控制器富有挑战性，因为它必须满足STP和L/ODMA的带宽要求，还要将它们的所有读写操作调度给物理SRAM存储体。LBP控制器是Pixel Visoal Core最复杂的部分之一。
